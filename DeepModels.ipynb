{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5k8Mcoezp_m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import random\n",
    "import os\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "sequences = []\n",
    "with open(\"B_Enhancer.txt\", \"r\") as file:\n",
    "    seq = \"\"\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            if seq:\n",
    "                sequences.append(seq)\n",
    "                seq = \"\"\n",
    "        else:\n",
    "            seq += line\n",
    "    if seq:\n",
    "        sequences.append(seq)\n",
    "\n",
    "labels = [1 for i in range(len(sequences))]\n",
    "with open(\"B_NonEnhancer.txt\", \"r\") as file:\n",
    "    seq = \"\"\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            if seq:\n",
    "                sequences.append(seq)\n",
    "                seq = \"\"\n",
    "        else:\n",
    "            seq += line\n",
    "    if seq:\n",
    "        sequences.append(seq)\n",
    "labels = labels + [0 for i in range(len(sequences) - len(labels))]\n",
    "\n",
    "new_sequences = []\n",
    "with open(\"I_Enhancer.txt\", \"r\") as file:\n",
    "    seq = \"\"\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            if seq:\n",
    "                new_sequences.append(seq)\n",
    "                seq = \"\"\n",
    "        else:\n",
    "            seq += line\n",
    "    if seq:\n",
    "        new_sequences.append(seq)\n",
    "\n",
    "\n",
    "new_labels = [1 for i in range(len(new_sequences))]\n",
    "\n",
    "\n",
    "with open(\"I_NonEnhancer.txt\", \"r\") as file:\n",
    "    seq = \"\"\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            if seq:\n",
    "                new_sequences.append(seq)\n",
    "                seq = \"\"\n",
    "        else:\n",
    "            seq += line\n",
    "    if seq:\n",
    "        new_sequences.append(seq)\n",
    "\n",
    "new_labels = new_labels + [0 for i in range(len(new_sequences) - len(new_labels))]\n",
    "\n",
    "\n",
    "def load_dna2vec(path, k=3):\n",
    "    embedding_dict = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            kmer = values[0]\n",
    "            if len(kmer) == k:\n",
    "                vector = np.array(values[1:], dtype=np.float32)\n",
    "                embedding_dict[kmer] = vector\n",
    "    return embedding_dict\n",
    "\n",
    "def kmer_tokenize_str(sequences, k=3):\n",
    "    kmer_seqs = []\n",
    "    for seq in sequences:\n",
    "        seq = seq.upper()\n",
    "        kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "        kmer_seqs.append(kmers)\n",
    "    return kmer_seqs\n",
    "\n",
    "def build_kmer_index(kmer_seqs):\n",
    "    kmer_set = set(k for seq in kmer_seqs for k in seq)\n",
    "    kmer_to_idx = {k: i+1 for i, k in enumerate(sorted(kmer_set))}\n",
    "    return kmer_to_idx\n",
    "\n",
    "def encode_kmers(kmer_seqs, kmer_to_idx):\n",
    "    encoded = np.zeros((len(kmer_seqs), len(kmer_seqs[0])), dtype=np.int32)\n",
    "    for i, seq in enumerate(kmer_seqs):\n",
    "        for j, kmer in enumerate(seq):\n",
    "            encoded[i, j] = kmer_to_idx.get(kmer, 0)\n",
    "    return encoded\n",
    "\n",
    "def build_embedding_matrix(kmer_to_idx, embedding_dict, embedding_dim):\n",
    "    vocab_size = len(kmer_to_idx) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for kmer, idx in kmer_to_idx.items():\n",
    "        if kmer in embedding_dict:\n",
    "            embedding_matrix[idx] = embedding_dict[kmer]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.1, size=(embedding_dim,))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "k = 3\n",
    "embedding_dim = 100\n",
    "dna2vec_path = '/content/drive/MyDrive/dna2vec-20250728-1713-k3to8-100d-10c-20Mbp-sliding-5B7.w2v'\n",
    "\n",
    "kmer_seqs = kmer_tokenize_str(sequences, k=k)\n",
    "kmer_to_idx = build_kmer_index(kmer_seqs)\n",
    "\n",
    "embedding_dict = load_dna2vec(dna2vec_path, k=k)\n",
    "embedding_matrix = build_embedding_matrix(kmer_to_idx, embedding_dict, embedding_dim)\n",
    "encoded_seqs = encode_kmers(kmer_seqs, kmer_to_idx)\n",
    "\n",
    "kmer_seqs_new = kmer_tokenize_str(new_sequences, k=k)\n",
    "encoded_seqs_new = encode_kmers(kmer_seqs_new, kmer_to_idx)\n",
    "\n",
    "def seqs_to_embedded_vectors(encoded_seqs, embedding_matrix):\n",
    "    vectors = []\n",
    "    for seq in encoded_seqs:\n",
    "        emb = embedding_matrix[seq]\n",
    "        avg_emb = np.mean(emb, axis=0)\n",
    "        vectors.append(avg_emb)\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train = seqs_to_embedded_vectors(encoded_seqs, embedding_matrix)\n",
    "X_test = seqs_to_embedded_vectors(encoded_seqs_new, embedding_matrix)\n",
    "\n",
    "\n",
    "def performance(labelArr, predictArr):\n",
    "    TN, FP, FN, TP = metrics.confusion_matrix(labelArr, predictArr).ravel()\n",
    "    ACC = metrics.accuracy_score(labelArr, predictArr)\n",
    "    SN = metrics.recall_score(labelArr, predictArr)\n",
    "    SP = TN/(FP + TN)\n",
    "    MCC= matthews_corrcoef(labelArr, predictArr)\n",
    "    return ACC,SN,SP,MCC\n",
    "\n",
    "\n",
    "\n",
    "def compute_gc_content(seq):\n",
    "    seq = seq.upper()\n",
    "    gc_count = seq.count('G') + seq.count('C')\n",
    "    return gc_count / len(seq) if len(seq) > 0 else 0\n",
    "\n",
    "def compute_sequence_entropy(seq, k=3):\n",
    "    seq = seq.upper()\n",
    "    kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "    kmer_counts = Counter(kmers)\n",
    "    probs = [count / len(kmers) for count in kmer_counts.values()]\n",
    "    return entropy(probs)\n",
    "\n",
    "def extract_stat_features(seqs, k=3):\n",
    "    features = []\n",
    "    for seq in seqs:\n",
    "        gc = compute_gc_content(seq)\n",
    "        ent = compute_sequence_entropy(seq, k=k)\n",
    "        features.append([gc, ent])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def make_pwm_from_consensus(consensus):\n",
    "    instances = [Seq(consensus)]\n",
    "    m = motifs.create(instances)\n",
    "    pwm = m.counts.normalize(pseudocounts=0.1)\n",
    "    return pwm\n",
    "\n",
    "def pwm_score_features(sequences, motif_list):\n",
    "    pwm_logodds_list = []\n",
    "    for motif in motif_list:\n",
    "        pwm = make_pwm_from_consensus(motif)\n",
    "        log_odds = pwm.log_odds()\n",
    "        pwm_logodds_list.append(log_odds)\n",
    "\n",
    "    features = []\n",
    "    for seq in sequences:\n",
    "        seq = Seq(seq.upper())\n",
    "        seq_scores = []\n",
    "        for log_odds in pwm_logodds_list:\n",
    "            scores = [score for _, score in log_odds.search(seq)]\n",
    "            max_score = max(scores) if scores else 0.0\n",
    "            seq_scores.append(max_score)\n",
    "        features.append(seq_scores)\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "\n",
    "def get_all_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_frequency_features(sequences, k=3):\n",
    "    all_kmers = get_all_kmers(k)\n",
    "    features = []\n",
    "    for seq in sequences:\n",
    "        seq = seq.upper()\n",
    "        kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "        kmer_counts = Counter(kmers)\n",
    "        total = sum(kmer_counts.values())\n",
    "        freq_vector = [kmer_counts[kmer] / total if total > 0 else 0.0 for kmer in all_kmers]\n",
    "        features.append(freq_vector)\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def dinuc_freq(sequences):\n",
    "    dinucs = [a+b for a in 'ACGT' for b in 'ACGT']\n",
    "    features = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = seq.upper()\n",
    "        total = len(seq) - 1\n",
    "        counts = Counter([seq[i:i+2] for i in range(total)])\n",
    "        freq = [counts[d]/total if total > 0 else 0 for d in dinucs]\n",
    "        features.append(freq)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "X_train_dinuc = dinuc_freq(sequences)\n",
    "X_test_dinuc = dinuc_freq(new_sequences)\n",
    "\n",
    "# motifs_list = ['TATA', 'CAAT','GATA']\n",
    "motifs_list = ['TATA','CCCTG','CCTGG','GCCTG','CCCAGG','CCCAGCC','TTGGGAG']\n",
    "motif_features_train = pwm_score_features(sequences, motifs_list)\n",
    "motif_features_test = pwm_score_features(new_sequences, motifs_list)\n",
    "\n",
    "stat_features_train = extract_stat_features(sequences, k=3)\n",
    "stat_features_test = extract_stat_features(new_sequences, k=3)\n",
    "\n",
    "kmer_freq_train = kmer_frequency_features(sequences, k=3)\n",
    "kmer_freq_test = kmer_frequency_features(new_sequences, k=3)\n",
    "\n",
    "X_train = np.concatenate([X_train, kmer_freq_train], axis=1)\n",
    "X_test = np.concatenate([X_test, kmer_freq_test], axis=1)\n",
    "X_train = np.concatenate([X_train, stat_features_train], axis=1)\n",
    "X_test = np.concatenate([X_test, stat_features_test], axis=1)\n",
    "X_train = np.concatenate([X_train, motif_features_train], axis=1)\n",
    "X_test = np.concatenate([X_test, motif_features_test], axis=1)\n",
    "X_train = np.concatenate([X_train, X_train_dinuc], axis=1)\n",
    "X_test = np.concatenate([X_test, X_test_dinuc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmElU3kJ7a1S"
   },
   "outputs": [],
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnCwb1DT1gOb",
    "outputId": "f1162871-53e1-44a5-f249-8b092f526317"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(labels)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(new_labels)\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(128, 3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "        Conv1D(64, 3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_cnn_model((X_train.shape[1], 1))\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, validation_split=0.2, epochs=30,\n",
    "    batch_size=64, callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "ACC = accuracy_score(y_test, y_pred_test)\n",
    "SN = recall_score(y_test, y_pred_test)\n",
    "SP = TN / (TN + FP)\n",
    "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"Accuracy: {ACC:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {SN:.4f}\")\n",
    "print(f\"Specificity: {SP:.4f}\")\n",
    "print(f\"MCC: {MCC:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWqrZiL-7u_V"
   },
   "source": [
    "CNN+attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IvJenpCR74gU",
    "outputId": "8b4bfacc-80a2-48de-ae68-ac398f494662"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization,\n",
    "    MultiHeadAttention, Add, LayerNormalization, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(labels)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(new_labels)\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "def build_advanced_cnn_transformer(input_shape,\n",
    "                                   num_heads=8,\n",
    "                                   key_dim=64,\n",
    "                                   dropout_rate=0.4,\n",
    "                                   dense_units=256,\n",
    "                                   lr=1e-4):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(256, 7, activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = Dense(x.shape[-1])(attn_output) \n",
    "    x = Add()([x, attn_output])\n",
    "    x = LayerNormalization()(x)\n",
    "    ffn = Dense(dense_units, activation='relu')(x)\n",
    "    ffn = Dropout(dropout_rate)(ffn)\n",
    "    ffn = Dense(x.shape[-1])(ffn)\n",
    "    x = Add()([x, ffn])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_advanced_cnn_transformer(\n",
    "    input_shape=(X_train.shape[1], 1),\n",
    "    num_heads=11,\n",
    "    key_dim=64,\n",
    "    dropout_rate=0.3,\n",
    "    dense_units=256,\n",
    "    lr=1e-4\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor='accuracy', patience=6, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50, batch_size=64, callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "ACC = accuracy_score(y_test, y_pred_test)\n",
    "SN = recall_score(y_test, y_pred_test)\n",
    "SP = TN / (TN + FP)\n",
    "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\n Test Set Results:\")\n",
    "print(f\"Accuracy: {ACC:.4f}\")\n",
    "print(f\"Sensitivity (SN): {SN:.4f}\")\n",
    "print(f\"Specificity (SP): {SP:.4f}\")\n",
    "print(f\"MCC: {MCC:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJnW1mHpM7ry"
   },
   "source": [
    "lstm+cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2X3adyMNBep",
    "outputId": "1a441e59-28b2-486e-b5c2-0a97859b4c3a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, Flatten, Dense, Dropout,\n",
    "    BatchNormalization, LSTM, Bidirectional\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(labels)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(new_labels)\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "def build_cnn_lstm_model(input_shape, lstm_units=128, dense_units=128,\n",
    "                         dropout_rate=0.4, lr=1e-4):\n",
    "    model = Sequential([\n",
    "        Conv1D(128, 5, activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "        Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=False, dropout=0.3, recurrent_dropout=0.2)),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_cnn_lstm_model(\n",
    "    input_shape=(X_train.shape[1], 1), lstm_units=32,\n",
    "    dense_units=128,\n",
    "    dropout_rate=0.3, lr=1e-4 \n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='accuracy', patience=5, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='accuracy', factor=0.5, patience=3, verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "ACC = accuracy_score(y_test, y_pred_test)\n",
    "SN = recall_score(y_test, y_pred_test)\n",
    "SP = TN / (TN + FP)\n",
    "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\n Test Set Results:\")\n",
    "print(f\"Accuracy: {ACC:.4f}\")\n",
    "print(f\"Sensitivity (SN): {SN:.4f}\")\n",
    "print(f\"Specificity (SP): {SP:.4f}\")\n",
    "print(f\"MCC: {MCC:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZyv4280gDUP"
   },
   "source": [
    "Blstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FntKVrpJM7Sj",
    "outputId": "6be85808-92df-47af-9abe-98684bedf775"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(labels)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(new_labels)\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "def build_bilstm_model(input_shape, lr=0.001):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(32, return_sequences=True), input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Bidirectional(LSTM(64)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_bilstm_model((X_train.shape[1], 1))\n",
    "\n",
    "early_stop = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.5, patience=3, min_lr=1e-4, verbose=1)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "ACC = accuracy_score(y_test, y_pred_test)\n",
    "SN = recall_score(y_test, y_pred_test)\n",
    "SP = TN / (TN + FP)\n",
    "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\n Test Set Results:\")\n",
    "print(f\"Accuracy: {ACC:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {SN:.4f}\")\n",
    "print(f\"Specificity: {SP:.4f}\")\n",
    "print(f\"MCC: {MCC:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFIGrdSaAjZv"
   },
   "source": [
    "lstm cnn attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ujDcse5M7It",
    "outputId": "76d0ea75-2b1b-4612-c59d-3a66254895a6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, BatchNormalization, Dropout,\n",
    "    Bidirectional, LSTM, Dense, Flatten, Layer\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        output = tf.keras.backend.sum(x * a, axis=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "X_train = np.expand_dims(np.array(X_train), axis=2)\n",
    "X_test = np.expand_dims(np.array(X_test), axis=2)\n",
    "y_train = np.array(labels)\n",
    "y_test = np.array(new_labels)\n",
    "def build_cnn_lstm_attention(input_shape, lstm_units=32, dense_units=128,\n",
    "                             dropout_rate=0.3, lr=1e-4):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(128, 5, activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv1D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True,\n",
    "                           dropout=0.3, recurrent_dropout=0.2))(x)\n",
    "    x = Attention()(x)\n",
    "\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_cnn_lstm_attention(\n",
    "    input_shape=(X_train.shape[1], 1),\n",
    "    lstm_units=32,\n",
    "    dense_units=128,\n",
    "    dropout_rate=0.3,\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='accuracy', patience=5, restore_best_weights=True, verbose=1\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='accuracy', factor=0.5, patience=3, verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "ACC = accuracy_score(y_test, y_pred_test)\n",
    "SN = recall_score(y_test, y_pred_test)\n",
    "SP = TN / (TN + FP)\n",
    "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\n Test Set Results:\")\n",
    "print(f\"Accuracy: {ACC:.4f}\")\n",
    "print(f\"Sensitivity (SN): {SN:.4f}\")\n",
    "print(f\"Specificity (SP): {SP:.4f}\")\n",
    "print(f\"MCC: {MCC:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
